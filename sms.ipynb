{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IlelW9LDmO22"
   },
   "source": [
    "### Load the dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2058,
     "status": "ok",
     "timestamp": 1547993435499,
     "user": {
      "displayName": "SUNEEL G",
      "photoUrl": "",
      "userId": "05702681006049335082"
     },
     "user_tz": -330
    },
    "id": "hwUx7EwTmO23",
    "outputId": "6dd6711a-805c-4572-dcea-037132de4dc1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SUNEEL\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Import the keras libraries\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dense, LSTM, Dropout\n",
    "from keras.layers.embeddings import Embedding #To convert an integer to embedding\n",
    "from keras.preprocessing import sequence #To convert a variable length sentence into a prespecified length\n",
    "# fix random seed for reproducibility\n",
    "np.random.seed(7)\n",
    "from keras.utils.np_utils import to_categorical\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Jx505aDxmO2_"
   },
   "outputs": [],
   "source": [
    "# Import the sklearn libraries\n",
    "import re\n",
    "import sklearn\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 883,
     "status": "ok",
     "timestamp": 1547993662853,
     "user": {
      "displayName": "SUNEEL G",
      "photoUrl": "",
      "userId": "05702681006049335082"
     },
     "user_tz": -330
    },
    "id": "l1WOLFQFmO3E",
    "outputId": "c5a83fc8-2e6a-4a23-ad96-63d60c892c70"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\SUNEEL'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the relevant path in your local machine\n",
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t2Bou4Q5orLE"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 881
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 935,
     "status": "error",
     "timestamp": 1547994180519,
     "user": {
      "displayName": "SUNEEL G",
      "photoUrl": "",
      "userId": "05702681006049335082"
     },
     "user_tz": -330
    },
    "id": "xp9AeHgYmO3H",
    "outputId": "c02c57f8-c90e-47b8-9cbb-71bcab85fe9c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Label                                            Message\n",
      "0       ham                 oh how abt 2 days before Christmas\n",
      "1      info  Welcome to OVATION HOLD R.No. 184, 114, 395, 3...\n",
      "2      info  Thank you for using your ICICI bank CREDITcard...\n",
      "3       ham  schedule a meeting with the entire team in the...\n",
      "4       ham                                Tommy is my brother\n",
      "5      spam  OTP is 817453 for the txn of INR 8262.00 at SP...\n",
      "6       ham                   the meeting is scheduled by john\n",
      "7      spam  Dear customer, We wish you a Merry Christmas. ...\n",
      "8      spam  Delivered: Your package withPawzone Red 1.25 i...\n",
      "9      info  The PNR for your Air India Flt 7I115 for PGH-B...\n",
      "10     info  Bimal Auto Agency : Service of your car KA52C8...\n",
      "11     info  Appointment with Dr Clayton in Pune on 2011-08...\n",
      "12     info  Maha Veer Auto Agency : Service of your car KA...\n",
      "13     spam  Dear AirAsia Customer, flight 5Q658 from RJA s...\n",
      "14     info  Dear Guest, Thanks for choosing Forlini's Rest...\n",
      "15      ham                          I will indeed! What time?\n",
      "16     info  Dear Guest, Thanks for choosing 2nd Avenue Del...\n",
      "17     info  Aryan Auto Agency : Service of your car KA87A3...\n",
      "18     info  Welcome to China Shipbuilding Industry R.No. 4...\n",
      "19      ham                  i will meet you on coming morning\n",
      "20     info  * KSRTC m-Ticket *  from: SRINGERI to: Pune Ps...\n",
      "21     info  PNR:3753775534,TRAIN:23181,DOJ:2013-01-02,AC3,...\n",
      "22      ham                i will meet john adams in bangalore\n",
      "23      ham   I have to catchup for dinner @ nite at his place\n",
      "24      ham                        lets meet 29th after diwali\n",
      "25     info  Repair ref.no for your car is JC53251731 opene...\n",
      "26     spam  Ur transaction on HDFC Bank CREDIT Card ending...\n",
      "27     info  [Cranks] Hi Joseph, your reservation is confir...\n",
      "28     info  Aryan Auto Agency : Service of your car KA17O3...\n",
      "29     spam  Roaming Info: Opt for International Roaming pa...\n",
      "...     ...                                                ...\n",
      "29970   ham                          meet for party over movie\n",
      "29971  info  Hi Customer, Booking ID: WZY37U5. Seats: STAND...\n",
      "29972  info  Dear Guest, Thanks for choosing Tastte!. Order...\n",
      "29973  info  Welcome to us polo R.No. 369, 438 Ch.In 2011-0...\n",
      "29974  info  Thank you for choosing Uber for 2010-01-05 at ...\n",
      "29975  info  YourBus (AGUMBE-KIAL 19:10) left  Jalahalli Cr...\n",
      "29976  info  Repair ref.no for your car is JC45757532 opene...\n",
      "29977   ham                            I'm am kind of busy now\n",
      "29978  spam  Delivered: Your package with 3.3V &amp; 5V Pow...\n",
      "29979  info  * KSRTC m-Ticket *  from: THIRUNALLAR to: PUTT...\n",
      "29980  spam  Delivered: Your package withPrestige PGMFB 800...\n",
      "29981  info  Repair ref.no for your car is JC63858438 opene...\n",
      "29982   ham  What time did you want me to drop you off at t...\n",
      "29983  info  Repair ref.no for your car is JC75514723 opene...\n",
      "29984   ham                                   2nd of September\n",
      "29985  info  [Coffeeshop Company] Hi Mario, your reservatio...\n",
      "29986  info  * KSRTC m-Ticket *  from: SRIKALAHASTI to: ANW...\n",
      "29987   ham          Hi..Please bring 1 ltr bottle of Pepsi ..\n",
      "29988   ham         there is a 40 minutes meeting starting 3pm\n",
      "29989   ham  Call in the hospital number between 5 and 6 an...\n",
      "29990  info  * KSRTC m-Ticket *  from: ALIKE to: Panaji Goa...\n",
      "29991   ham                                   Ok , c u there..\n",
      "29992  info  The PNR for your INDIGO Flt 8R176 for PAT-HJR ...\n",
      "29993  info  [Brewers Fayre] Hi Tristan, your reservation i...\n",
      "29994  spam  Dear AirAsia Customer, flight 2D866 from PAT s...\n",
      "29995   ham                               ok that works for me\n",
      "29996  spam  Delivered: Your package withSri High Quality S...\n",
      "29997   ham                      Let us dine at the Taj on Sat\n",
      "29998  spam  Delivered: Your package withSeCro (Pack of 2) ...\n",
      "29999  spam  OTP is 676348 for the txn of INR 6540.00 at Ar...\n",
      "\n",
      "[30000 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Read the data\n",
    "\n",
    "sms_data = pd.read_csv('TRAIN_SMS.csv')\n",
    "print(sms_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WNZOoTlGmO3L"
   },
   "outputs": [],
   "source": [
    "# Simple definition to process the words further\n",
    "\n",
    "def message_to_words(raw_message):\n",
    "#     letters_only = re.sub(\"[^a-zA-Z]\", \" \", raw_converse) \n",
    "    # 1. Lower case & split  \n",
    "    words = raw_message.lower().split()                             \n",
    "    # 2. Convert stop words to a set\n",
    "    stops = set(stopwords.words(\"english\"))                  \n",
    "    # 3. Remove stop words\n",
    "    meaningful_words = [w for w in words if not w in stops]   \n",
    "    # 4. Join the words back into one string\n",
    "    return(b\" \".join(meaningful_words)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A9psV0LKmO3O"
   },
   "outputs": [],
   "source": [
    "# Checking the length of the column\n",
    "num_message = sms_data[\"Message\"].size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Jo58WSTzmO3R",
    "outputId": "0de910e1-6041-48d7-8743-f66c71780409"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30000\n"
     ]
    }
   ],
   "source": [
    "print(num_message)\n",
    "#print(num_message[:3], Label[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0fXYfG8qmO3U"
   },
   "outputs": [],
   "source": [
    "# Initialize empty directories\n",
    "\n",
    "Message = []\n",
    "Label = [] # First target\n",
    "\n",
    "for x in range(len(sms_data.Message)):\n",
    "    Message.append(message_to_words(str(sms_data.Message[x]).encode('ascii','ignore')))\n",
    "    Label.append(sms_data.Label[x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wHJyURQ_mO3Y"
   },
   "source": [
    "### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jJmHpNT1mO3Y",
    "outputId": "eea294c8-99f1-4c9b-9efe-fe1d74fe7173"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['00',\n",
       " '000',\n",
       " '01',\n",
       " '02',\n",
       " '03',\n",
       " '04',\n",
       " '05',\n",
       " '06',\n",
       " '07',\n",
       " '08',\n",
       " '09',\n",
       " '10',\n",
       " '100',\n",
       " '1000',\n",
       " '10am',\n",
       " '10hrs',\n",
       " '10th',\n",
       " '11',\n",
       " '11am',\n",
       " '11hrs',\n",
       " '12',\n",
       " '121',\n",
       " '12hrs',\n",
       " '13',\n",
       " '13hrs',\n",
       " '14',\n",
       " '14hrs',\n",
       " '14th',\n",
       " '15',\n",
       " '15hrs',\n",
       " '15th',\n",
       " '16',\n",
       " '16hrs',\n",
       " '17',\n",
       " '17hrs',\n",
       " '17th',\n",
       " '18',\n",
       " '1800',\n",
       " '18hrs',\n",
       " '18th',\n",
       " '19',\n",
       " '19hrs',\n",
       " '19th',\n",
       " '1st',\n",
       " '20',\n",
       " '200',\n",
       " '2009',\n",
       " '2010',\n",
       " '2011',\n",
       " '2012',\n",
       " '2013',\n",
       " '2014',\n",
       " '2015',\n",
       " '2016',\n",
       " '2017',\n",
       " '20hrs',\n",
       " '20th',\n",
       " '21',\n",
       " '21hrs',\n",
       " '22',\n",
       " '22hrs',\n",
       " '23',\n",
       " '23hrs',\n",
       " '24',\n",
       " '24hrs',\n",
       " '25',\n",
       " '250',\n",
       " '25hrs',\n",
       " '26',\n",
       " '26hrs',\n",
       " '27',\n",
       " '27hrs',\n",
       " '28',\n",
       " '28hrs',\n",
       " '29',\n",
       " '29hrs',\n",
       " '2mrw',\n",
       " '2nd',\n",
       " '2pm',\n",
       " '30',\n",
       " '300',\n",
       " '30am',\n",
       " '30hrs',\n",
       " '30pm',\n",
       " '31',\n",
       " '31hrs',\n",
       " '31st',\n",
       " '32',\n",
       " '32hrs',\n",
       " '33',\n",
       " '33hrs',\n",
       " '34',\n",
       " '34hrs',\n",
       " '35',\n",
       " '35hrs',\n",
       " '36',\n",
       " '36hrs',\n",
       " '37',\n",
       " '37hrs',\n",
       " '38',\n",
       " '38hrs',\n",
       " '39',\n",
       " '39hrs',\n",
       " '3g',\n",
       " '3pm',\n",
       " '3rd',\n",
       " '40',\n",
       " '40hrs',\n",
       " '41',\n",
       " '41hrs',\n",
       " '42',\n",
       " '42hrs',\n",
       " '43',\n",
       " '43hrs',\n",
       " '44',\n",
       " '44hrs',\n",
       " '45',\n",
       " '45hrs',\n",
       " '46',\n",
       " '46hrs',\n",
       " '47',\n",
       " '47hrs',\n",
       " '48',\n",
       " '48hrs',\n",
       " '49',\n",
       " '49hrs',\n",
       " '4k',\n",
       " '4pm',\n",
       " '4th',\n",
       " '50',\n",
       " '500',\n",
       " '50hrs',\n",
       " '51',\n",
       " '51hrs',\n",
       " '52',\n",
       " '52hrs',\n",
       " '53',\n",
       " '53hrs',\n",
       " '54',\n",
       " '55',\n",
       " '55hrs',\n",
       " '56',\n",
       " '56hrs',\n",
       " '57',\n",
       " '57hrs',\n",
       " '58',\n",
       " '58hrs',\n",
       " '59',\n",
       " '5pm',\n",
       " '5th',\n",
       " '60',\n",
       " '61',\n",
       " '62',\n",
       " '63',\n",
       " '64',\n",
       " '65',\n",
       " '66',\n",
       " '67',\n",
       " '68',\n",
       " '69',\n",
       " '6pm',\n",
       " '70',\n",
       " '72',\n",
       " '73',\n",
       " '74',\n",
       " '75',\n",
       " '76',\n",
       " '77',\n",
       " '78',\n",
       " '7967',\n",
       " '7pm',\n",
       " '80',\n",
       " '8066',\n",
       " '81',\n",
       " '82',\n",
       " '85',\n",
       " '86',\n",
       " '87',\n",
       " '88',\n",
       " '8pm',\n",
       " '8th',\n",
       " '9pm',\n",
       " 'a10',\n",
       " 'a11',\n",
       " 'a13',\n",
       " 'a15',\n",
       " 'a16',\n",
       " 'aaron',\n",
       " 'abhanshu',\n",
       " 'about',\n",
       " 'above',\n",
       " 'abt',\n",
       " 'ac',\n",
       " 'ac3',\n",
       " 'account',\n",
       " 'ad',\n",
       " 'add',\n",
       " 'added',\n",
       " 'adrian',\n",
       " 'after',\n",
       " 'afternoon',\n",
       " 'again',\n",
       " 'agara',\n",
       " 'age',\n",
       " 'agency',\n",
       " 'agr',\n",
       " 'agumbe',\n",
       " 'agx',\n",
       " 'air',\n",
       " 'airasia',\n",
       " 'aircel',\n",
       " 'airport',\n",
       " 'airporttaxi',\n",
       " 'airtel',\n",
       " 'airways',\n",
       " 'ajl',\n",
       " 'akd',\n",
       " 'alex',\n",
       " 'alexander',\n",
       " 'alike',\n",
       " 'all',\n",
       " 'along',\n",
       " 'already',\n",
       " 'alright',\n",
       " 'also',\n",
       " 'am',\n",
       " 'amazon',\n",
       " 'amd',\n",
       " 'amount',\n",
       " 'amp',\n",
       " 'amt',\n",
       " 'an',\n",
       " 'anand',\n",
       " 'and',\n",
       " 'andy',\n",
       " 'anniversary',\n",
       " 'anwatti',\n",
       " 'any',\n",
       " 'anyone',\n",
       " 'app',\n",
       " 'apply',\n",
       " 'appointment',\n",
       " 'approx',\n",
       " 'appt',\n",
       " 'apr',\n",
       " 'april',\n",
       " 'arcade',\n",
       " 'are',\n",
       " 'arena',\n",
       " 'around',\n",
       " 'aryan',\n",
       " 'as',\n",
       " 'ask',\n",
       " 'askme',\n",
       " 'assigned',\n",
       " 'assistance',\n",
       " 'at',\n",
       " 'atq',\n",
       " 'attend',\n",
       " 'attending',\n",
       " 'attibele',\n",
       " 'auto',\n",
       " 'avail',\n",
       " 'available',\n",
       " 'avbl',\n",
       " 'avl',\n",
       " 'axis',\n",
       " 'b1',\n",
       " 'b2',\n",
       " 'b3',\n",
       " 'b4',\n",
       " 'b5',\n",
       " 'b6',\n",
       " 'b7',\n",
       " 'b8',\n",
       " 'b9',\n",
       " 'baby',\n",
       " 'back',\n",
       " 'bad',\n",
       " 'bag',\n",
       " 'bajaj',\n",
       " 'bal',\n",
       " 'balance',\n",
       " 'banashankari',\n",
       " 'banaswadi',\n",
       " 'bangalore',\n",
       " 'bank',\n",
       " 'bannerghatta',\n",
       " 'bar',\n",
       " 'basis',\n",
       " 'bazaar',\n",
       " 'bbi',\n",
       " 'bda',\n",
       " 'bdq',\n",
       " 'be',\n",
       " 'beach',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'belagavi',\n",
       " 'bellandur',\n",
       " 'bengaluru',\n",
       " 'benjamin',\n",
       " 'bep',\n",
       " 'best',\n",
       " 'between',\n",
       " 'bhj',\n",
       " 'bho',\n",
       " 'bhu',\n",
       " 'big',\n",
       " 'bill',\n",
       " 'bimal',\n",
       " 'binnypet',\n",
       " 'birthday',\n",
       " 'bit',\n",
       " 'bk',\n",
       " 'bkb',\n",
       " 'black',\n",
       " 'blessed',\n",
       " 'blr',\n",
       " 'blue',\n",
       " 'boarding',\n",
       " 'boardingpt',\n",
       " 'bom',\n",
       " 'bommanahalli',\n",
       " 'book',\n",
       " 'booking',\n",
       " 'box8',\n",
       " 'brandon',\n",
       " 'brayden',\n",
       " 'breakfast',\n",
       " 'brian',\n",
       " 'bridge',\n",
       " 'brigade',\n",
       " 'bring',\n",
       " 'broadband',\n",
       " 'brookefield',\n",
       " 'brother',\n",
       " 'bryce',\n",
       " 'bsnl',\n",
       " 'bucks',\n",
       " 'buddy',\n",
       " 'bup',\n",
       " 'burger',\n",
       " 'bus',\n",
       " 'busy',\n",
       " 'but',\n",
       " 'buy',\n",
       " 'buzz',\n",
       " 'by',\n",
       " 'bz',\n",
       " 'cab',\n",
       " 'cafe',\n",
       " 'call',\n",
       " 'called',\n",
       " 'calls',\n",
       " 'camden',\n",
       " 'can',\n",
       " 'cancelled',\n",
       " 'cannot',\n",
       " 'car',\n",
       " 'card',\n",
       " 'care',\n",
       " 'carry',\n",
       " 'case',\n",
       " 'cash',\n",
       " 'cashback',\n",
       " 'catch',\n",
       " 'cauvery',\n",
       " 'cbfwh1e',\n",
       " 'cc',\n",
       " 'ccd',\n",
       " 'ccj',\n",
       " 'ccu',\n",
       " 'cdp',\n",
       " 'cell',\n",
       " 'center',\n",
       " 'central',\n",
       " 'centre',\n",
       " 'ch',\n",
       " 'chair',\n",
       " 'chamarajpet',\n",
       " 'channarayapatna',\n",
       " 'charles',\n",
       " 'chase',\n",
       " 'chaturthi',\n",
       " 'chauffer',\n",
       " 'check',\n",
       " 'chennai',\n",
       " 'chicken',\n",
       " 'chikkamagaluru',\n",
       " 'children',\n",
       " 'chitradurga',\n",
       " 'chk',\n",
       " 'choosing',\n",
       " 'christmas',\n",
       " 'chuseok',\n",
       " 'cinema',\n",
       " 'cinemas',\n",
       " 'cinepolis',\n",
       " 'circle',\n",
       " 'citibank',\n",
       " 'city',\n",
       " 'class',\n",
       " 'clayton',\n",
       " 'click',\n",
       " 'clinic',\n",
       " 'clock',\n",
       " 'closes1',\n",
       " 'closes10',\n",
       " 'closes11',\n",
       " 'closes12',\n",
       " 'closes13',\n",
       " 'closes14',\n",
       " 'closes15',\n",
       " 'closes16',\n",
       " 'closes17',\n",
       " 'closes18',\n",
       " 'closes19',\n",
       " 'closes20',\n",
       " 'closes21',\n",
       " 'closes22',\n",
       " 'closes23',\n",
       " 'closes25',\n",
       " 'closes26',\n",
       " 'closes27',\n",
       " 'closes28',\n",
       " 'closes29',\n",
       " 'closes3',\n",
       " 'closes4',\n",
       " 'closes5',\n",
       " 'closes6',\n",
       " 'closes7',\n",
       " 'closes8',\n",
       " 'closes9',\n",
       " 'clothes',\n",
       " 'club',\n",
       " 'cm',\n",
       " 'co',\n",
       " 'code',\n",
       " 'coffee',\n",
       " 'coimbatore',\n",
       " 'cok',\n",
       " 'colby',\n",
       " 'collection',\n",
       " 'college',\n",
       " 'collin',\n",
       " 'com',\n",
       " 'come',\n",
       " 'coming',\n",
       " 'company',\n",
       " 'complex',\n",
       " 'concerned',\n",
       " 'confirmed',\n",
       " 'contact',\n",
       " 'contact_name',\n",
       " 'cool',\n",
       " 'coonur',\n",
       " 'coroporation',\n",
       " 'could',\n",
       " 'course',\n",
       " 'cover',\n",
       " 'cpn',\n",
       " 'cream',\n",
       " 'create',\n",
       " 'credit',\n",
       " 'creditcard',\n",
       " 'credited',\n",
       " 'cross',\n",
       " 'curr',\n",
       " 'customer',\n",
       " 'd16',\n",
       " 'dae',\n",
       " 'daily',\n",
       " 'dairy',\n",
       " 'damian',\n",
       " 'dance',\n",
       " 'daniel',\n",
       " 'data',\n",
       " 'date',\n",
       " 'date_sent',\n",
       " 'dated',\n",
       " 'davanegere',\n",
       " 'day',\n",
       " 'days',\n",
       " 'dc',\n",
       " 'dear',\n",
       " 'debit',\n",
       " 'debitcard',\n",
       " 'debited',\n",
       " 'dec',\n",
       " 'ded',\n",
       " 'defaulting',\n",
       " 'del',\n",
       " 'delayed',\n",
       " 'delhi',\n",
       " 'delivered',\n",
       " 'delivery',\n",
       " 'dentist',\n",
       " 'dep',\n",
       " 'departure',\n",
       " 'deptime',\n",
       " 'details',\n",
       " 'dharmastala',\n",
       " 'dhm',\n",
       " 'dial',\n",
       " 'dib',\n",
       " 'did',\n",
       " 'digital',\n",
       " 'din',\n",
       " 'diner',\n",
       " 'dinner',\n",
       " 'disconnected',\n",
       " 'discount',\n",
       " 'discover',\n",
       " 'discuss',\n",
       " 'discussion',\n",
       " 'diu',\n",
       " 'diwali',\n",
       " 'dmu',\n",
       " 'do',\n",
       " 'docomo',\n",
       " 'doctor',\n",
       " 'does',\n",
       " 'dog',\n",
       " 'doj',\n",
       " 'dominick',\n",
       " 'dominos',\n",
       " 'domlur',\n",
       " 'don',\n",
       " 'done',\n",
       " 'dont',\n",
       " 'donuts',\n",
       " 'down',\n",
       " 'download',\n",
       " 'downtown',\n",
       " 'dr',\n",
       " 'drop',\n",
       " 'due',\n",
       " 'during',\n",
       " 'dusshera',\n",
       " 'dylan',\n",
       " 'early',\n",
       " 'east',\n",
       " 'eat',\n",
       " 'ebay',\n",
       " 'edwin',\n",
       " 'eid',\n",
       " 'electricity',\n",
       " 'electronic',\n",
       " 'elements',\n",
       " 'elijah',\n",
       " 'email',\n",
       " 'emergency',\n",
       " 'emi',\n",
       " 'end',\n",
       " 'ending',\n",
       " 'engagement',\n",
       " 'enjoy',\n",
       " 'enquire',\n",
       " 'ernakulam',\n",
       " 'est',\n",
       " 'etd',\n",
       " 'eve',\n",
       " 'evening',\n",
       " 'event',\n",
       " 'every',\n",
       " 'everyday',\n",
       " 'exclusive',\n",
       " 'extra',\n",
       " 'f1',\n",
       " 'f2',\n",
       " 'f3',\n",
       " 'f4',\n",
       " 'f5',\n",
       " 'f6',\n",
       " 'f7',\n",
       " 'f8',\n",
       " 'f9',\n",
       " 'falls',\n",
       " 'family',\n",
       " 'fare',\n",
       " 'fb',\n",
       " 'fc',\n",
       " 'feb',\n",
       " 'field',\n",
       " 'find',\n",
       " 'fine',\n",
       " 'finserv',\n",
       " 'first',\n",
       " 'fitr',\n",
       " 'fixedline',\n",
       " 'flat',\n",
       " 'flight',\n",
       " 'flipkart',\n",
       " 'flt',\n",
       " 'flyover',\n",
       " 'food',\n",
       " 'for',\n",
       " 'forget',\n",
       " 'forgot',\n",
       " 'forum',\n",
       " 'free',\n",
       " 'fresh',\n",
       " 'friday',\n",
       " 'friends',\n",
       " 'from',\n",
       " 'fun',\n",
       " 'game',\n",
       " 'gandhi',\n",
       " 'gandhinagar',\n",
       " 'gandinagar',\n",
       " 'ganesh',\n",
       " 'garuda',\n",
       " 'gate',\n",
       " 'gau',\n",
       " 'gay',\n",
       " 'gb',\n",
       " 'generated',\n",
       " 'george',\n",
       " 'gerardo',\n",
       " 'get',\n",
       " 'girl',\n",
       " 'give',\n",
       " 'gl',\n",
       " 'glass',\n",
       " 'go',\n",
       " 'goa',\n",
       " 'going',\n",
       " 'gold',\n",
       " 'gonna',\n",
       " 'goo',\n",
       " 'good',\n",
       " 'gop',\n",
       " 'gopalan',\n",
       " 'goragunte',\n",
       " 'got',\n",
       " 'gotta',\n",
       " 'grab',\n",
       " 'grand',\n",
       " 'great',\n",
       " 'greet',\n",
       " 'greetings',\n",
       " 'griffin',\n",
       " 'grill',\n",
       " 'group',\n",
       " 'gt',\n",
       " 'guest',\n",
       " 'gwl',\n",
       " 'gym',\n",
       " 'had',\n",
       " 'half',\n",
       " 'hall',\n",
       " 'happy',\n",
       " 'harrison',\n",
       " 'harry',\n",
       " 'has',\n",
       " 'hassan',\n",
       " 'have',\n",
       " 'hbx',\n",
       " 'hdfc',\n",
       " 'hdfcbank',\n",
       " 'he',\n",
       " 'health',\n",
       " 'heavy',\n",
       " 'hebbal',\n",
       " 'hello',\n",
       " 'help',\n",
       " 'henry',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hey',\n",
       " 'hi',\n",
       " 'high',\n",
       " 'him',\n",
       " 'hiriyur',\n",
       " 'his',\n",
       " 'hjr',\n",
       " 'hold',\n",
       " 'holi',\n",
       " 'home',\n",
       " 'honey',\n",
       " 'hoodi',\n",
       " 'hospital',\n",
       " 'hosur',\n",
       " 'hot',\n",
       " 'hotel',\n",
       " 'hour',\n",
       " 'hours',\n",
       " 'house',\n",
       " 'how',\n",
       " 'hrs',\n",
       " 'hsr',\n",
       " 'http',\n",
       " 'https',\n",
       " 'hubli',\n",
       " 'hundimala',\n",
       " 'hurry',\n",
       " 'hut',\n",
       " 'hv',\n",
       " 'hyd',\n",
       " 'hyderabad',\n",
       " 'hyundai',\n",
       " 'i13',\n",
       " 'ice',\n",
       " 'icici',\n",
       " 'id',\n",
       " 'idbi',\n",
       " 'idea',\n",
       " 'idr',\n",
       " 'if',\n",
       " 'imf',\n",
       " 'imps',\n",
       " 'in',\n",
       " 'independence',\n",
       " 'india',\n",
       " 'indigo',\n",
       " 'indira',\n",
       " 'info',\n",
       " 'inform',\n",
       " 'innovative',\n",
       " 'inox',\n",
       " 'inr',\n",
       " 'inr1',\n",
       " 'ins',\n",
       " 'intelligence',\n",
       " 'interest',\n",
       " 'international',\n",
       " 'internet',\n",
       " 'interview',\n",
       " 'invite',\n",
       " 'ips',\n",
       " 'is',\n",
       " 'isaac',\n",
       " 'isk',\n",
       " 'israel',\n",
       " 'it',\n",
       " 'item',\n",
       " 'items',\n",
       " 'itemswas',\n",
       " 'itemwas',\n",
       " 'its',\n",
       " 'ixe',\n",
       " 'ixm',\n",
       " 'j5',\n",
       " 'j6',\n",
       " 'j7',\n",
       " 'j8',\n",
       " 'j9',\n",
       " 'jabong',\n",
       " 'jack',\n",
       " 'jai',\n",
       " 'jalahalli',\n",
       " 'james',\n",
       " 'jan',\n",
       " 'janmashtami',\n",
       " 'january',\n",
       " 'jared',\n",
       " 'jason',\n",
       " 'jaxon',\n",
       " 'jaya',\n",
       " 'jayanagar',\n",
       " 'jayanti',\n",
       " 'jayden',\n",
       " 'jdh',\n",
       " 'jesus',\n",
       " 'jet',\n",
       " 'jga',\n",
       " 'jgb',\n",
       " 'jio',\n",
       " 'jlr',\n",
       " 'jnydate',\n",
       " 'john',\n",
       " 'johnny',\n",
       " 'join',\n",
       " 'jonathan',\n",
       " 'jordan',\n",
       " 'joseph',\n",
       " 'journey',\n",
       " 'jp',\n",
       " 'jrh',\n",
       " 'jsa',\n",
       " 'jul',\n",
       " 'julian',\n",
       " 'july',\n",
       " 'jun',\n",
       " 'junction',\n",
       " 'june',\n",
       " 'jungle',\n",
       " 'just',\n",
       " 'kailash',\n",
       " 'kalasipalayam',\n",
       " 'kannur',\n",
       " 'karaikudi',\n",
       " 'kasargod',\n",
       " 'kasturinagar',\n",
       " 'keep',\n",
       " 'kenneth',\n",
       " 'kg',\n",
       " 'kial',\n",
       " 'king',\n",
       " 'klh',\n",
       " 'know',\n",
       " 'kodaikanal',\n",
       " 'kolar',\n",
       " 'kolhapur',\n",
       " 'kottayam',\n",
       " 'kozhikode',\n",
       " 'ksrtc',\n",
       " 'ktu',\n",
       " 'kumbakonam',\n",
       " 'kundapura',\n",
       " 'la',\n",
       " 'lakshmi',\n",
       " 'lane',\n",
       " 'last',\n",
       " 'late',\n",
       " 'later',\n",
       " 'latest',\n",
       " 'laundry',\n",
       " 'lawyer',\n",
       " 'layout',\n",
       " 'lda',\n",
       " 'leave',\n",
       " 'left',\n",
       " 'let',\n",
       " 'lets',\n",
       " 'levi',\n",
       " 'library',\n",
       " 'lido',\n",
       " 'life',\n",
       " 'lights',\n",
       " 'like',\n",
       " 'limit',\n",
       " 'little',\n",
       " 'lko',\n",
       " 'll',\n",
       " 'loan',\n",
       " 'local',\n",
       " 'location',\n",
       " 'locked',\n",
       " 'london',\n",
       " 'long',\n",
       " 'looking',\n",
       " 'love',\n",
       " 'ltd',\n",
       " 'lucas',\n",
       " 'luh',\n",
       " 'lunch',\n",
       " 'ly',\n",
       " 'm7',\n",
       " 'made',\n",
       " 'madikeri',\n",
       " 'madras',\n",
       " 'madurai',\n",
       " 'magadi',\n",
       " 'magrath',\n",
       " 'maha',\n",
       " 'mahadevapura',\n",
       " 'make',\n",
       " 'makemytrip',\n",
       " 'mall',\n",
       " 'mandya',\n",
       " 'mangaluru',\n",
       " 'manipal',\n",
       " 'mantralaya',\n",
       " 'mantri',\n",
       " 'mar',\n",
       " 'marathahalli',\n",
       " 'march',\n",
       " 'market',\n",
       " 'marriage',\n",
       " 'martin',\n",
       " 'maruti',\n",
       " 'mary',\n",
       " 'max',\n",
       " 'maximum',\n",
       " 'maxwell',\n",
       " 'may',\n",
       " 'me',\n",
       " 'meenakshi',\n",
       " 'meet',\n",
       " 'meeting',\n",
       " 'member',\n",
       " 'men',\n",
       " 'mercara',\n",
       " 'merry',\n",
       " 'meru',\n",
       " 'message',\n",
       " 'messages',\n",
       " 'mg',\n",
       " 'michael',\n",
       " 'might',\n",
       " 'mile',\n",
       " 'milk',\n",
       " 'min',\n",
       " 'miniplex',\n",
       " 'mins',\n",
       " 'minutes',\n",
       " 'miss',\n",
       " 'missed',\n",
       " 'mobile',\n",
       " 'mom',\n",
       " 'mon',\n",
       " 'monday',\n",
       " 'money',\n",
       " 'month',\n",
       " 'months',\n",
       " 'more',\n",
       " 'morning',\n",
       " 'movie',\n",
       " 'mr',\n",
       " 'mrs',\n",
       " 'msr',\n",
       " 'much',\n",
       " 'muharram',\n",
       " 'multiplex',\n",
       " 'mumbai',\n",
       " 'must',\n",
       " 'mwgvjp',\n",
       " 'my',\n",
       " 'myantra',\n",
       " 'myq',\n",
       " 'mysore',\n",
       " 'mysuru',\n",
       " 'mzu',\n",
       " 'nag',\n",
       " 'nagar',\n",
       " 'name',\n",
       " 'national',\n",
       " 'navami',\n",
       " 'ndc',\n",
       " 'near',\n",
       " 'need',\n",
       " 'neighbourhood',\n",
       " 'nellore',\n",
       " 'net',\n",
       " 'network',\n",
       " 'never',\n",
       " 'new',\n",
       " 'next',\n",
       " 'nicolas',\n",
       " 'night',\n",
       " 'nishu',\n",
       " 'nmb',\n",
       " 'no',\n",
       " 'non',\n",
       " 'noon',\n",
       " 'northstar',\n",
       " 'not',\n",
       " 'note',\n",
       " 'nov',\n",
       " 'now',\n",
       " 'null',\n",
       " 'number',\n",
       " 'oct',\n",
       " 'october',\n",
       " 'of',\n",
       " 'off',\n",
       " 'offer',\n",
       " 'offers',\n",
       " 'office',\n",
       " 'oh',\n",
       " 'ohh',\n",
       " 'ok',\n",
       " 'okay',\n",
       " 'ola',\n",
       " 'old',\n",
       " 'on',\n",
       " 'onam',\n",
       " 'once',\n",
       " 'one',\n",
       " 'online',\n",
       " 'only',\n",
       " 'ooty',\n",
       " 'open',\n",
       " 'opened',\n",
       " 'operator',\n",
       " 'opt',\n",
       " 'or',\n",
       " 'order',\n",
       " 'orders',\n",
       " 'orion',\n",
       " 'other',\n",
       " 'otp',\n",
       " 'our',\n",
       " 'out',\n",
       " 'over',\n",
       " 'pab',\n",
       " 'pack',\n",
       " 'package',\n",
       " 'paid',\n",
       " 'palakkad',\n",
       " 'palya',\n",
       " 'panaji',\n",
       " 'panchami',\n",
       " 'park',\n",
       " 'party',\n",
       " 'password',\n",
       " 'past',\n",
       " ...]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create word frequency matrix for every Message, \n",
    "#Apply Tfidf vectorizer-extracting upto 1400 features\n",
    "\n",
    "\n",
    "vectorizer = CountVectorizer(analyzer = \"word\",\\\n",
    "                             tokenizer = None,\\\n",
    "                             preprocessor = None,\n",
    "                             ngram_range=(1,1), # We are only interested in uni grams\n",
    "                             max_features=1400, # Limits to 1400 features\n",
    "                             stop_words = None) \n",
    "\n",
    "data_features = vectorizer.fit_transform(Message)\n",
    "type(data_features)\n",
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZojjMFsXmO3c",
    "outputId": "7216a83a-b12b-43a8-f320-0c15fdd76e7a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30000 30000\n",
      "['ham' 'info' 'spam']\n"
     ]
    }
   ],
   "source": [
    "print(len(Message), len(Label)) #30000 obs\n",
    "print(np.unique(Label)) # 3 different categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XtS635k1mO3f"
   },
   "outputs": [],
   "source": [
    "# Convert the data_features to array\n",
    "\n",
    "# data features for categories\n",
    "data_features = data_features.toarray()\n",
    "data_features = pd.DataFrame(data_features)\n",
    "data_features[\"Label\"] = sms_data[\"Label\"]\n",
    "data_features = data_features.sample(frac =1)\n",
    "\n",
    "# #data features for sub_categories\n",
    "\n",
    "# data_features[\"sub_categories\"] = health_data[\"sub_categories\"]\n",
    "# data_features = data_features.sample(frac =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Kn5Ra56XmO3p",
    "outputId": "85c4f5fe-5ab1-41c7-bae5-624e2bc36d71"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       0  1  2  3  4  5  6  7  8  9  ...    1391  1392  1393  1394  1395  \\\n",
      "1252   0  0  0  0  0  0  0  0  0  0  ...       0     0     0     1     0   \n",
      "10444  0  0  0  0  0  0  0  0  0  0  ...       0     0     0     0     0   \n",
      "8994   1  0  0  0  0  0  0  0  0  0  ...       0     0     0     0     1   \n",
      "\n",
      "       1396  1397  1398  1399  Label  \n",
      "1252      0     0     0     0   spam  \n",
      "10444     0     0     0     0    ham  \n",
      "8994      0     0     0     0   spam  \n",
      "\n",
      "[3 rows x 1401 columns]\n",
      "(30000, 1401)\n"
     ]
    }
   ],
   "source": [
    "# Verify the shape and head of data_features\n",
    "print(data_features[:3])\n",
    "print(data_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GfYyssHDmO3t",
    "outputId": "48576974-9db8-4a65-81ea-635088f78b0a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "print(type(sms_data))\n",
    "print(type(data_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4x1Ymwk1mO3z",
    "outputId": "63f6066d-af85-4d66-e61e-9ff18177ea6a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SUNEEL\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:18: UserWarning: Pandas doesn't allow columns to be created via a new attribute name - see https://pandas.pydata.org/pandas-docs/stable/indexing.html#attribute-access\n",
      "C:\\Users\\SUNEEL\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:19: UserWarning: Pandas doesn't allow columns to be created via a new attribute name - see https://pandas.pydata.org/pandas-docs/stable/indexing.html#attribute-access\n",
      "C:\\Users\\SUNEEL\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:22: UserWarning: Pandas doesn't allow columns to be created via a new attribute name - see https://pandas.pydata.org/pandas-docs/stable/indexing.html#attribute-access\n",
      "C:\\Users\\SUNEEL\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:23: UserWarning: Pandas doesn't allow columns to be created via a new attribute name - see https://pandas.pydata.org/pandas-docs/stable/indexing.html#attribute-access\n",
      "C:\\Users\\SUNEEL\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:26: UserWarning: Pandas doesn't allow columns to be created via a new attribute name - see https://pandas.pydata.org/pandas-docs/stable/indexing.html#attribute-access\n",
      "C:\\Users\\SUNEEL\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:27: UserWarning: Pandas doesn't allow columns to be created via a new attribute name - see https://pandas.pydata.org/pandas-docs/stable/indexing.html#attribute-access\n"
     ]
    }
   ],
   "source": [
    "# Split into 60% train, 20% val and 20% test\n",
    "\n",
    "def train_validate_test_split(df, train_percent=.6, validate_percent=.2, seed=None):\n",
    "    np.random.seed(seed)\n",
    "    perm = np.random.permutation(df.index)\n",
    "    m = len(df)\n",
    "    train_end = int(train_percent * m)\n",
    "    validate_end = int(validate_percent * m) + train_end\n",
    "    train = df.iloc[perm[:train_end]]\n",
    "    validate = df.iloc[perm[train_end:validate_end]]\n",
    "    test = df.iloc[perm[validate_end:]]\n",
    "    return train, validate, test\n",
    "\n",
    "#First split\n",
    "train, validate, test = train_validate_test_split(data_features) \n",
    "cols = [col for col in data_features.columns if col not in [\"Label\"]]\n",
    "\n",
    "train.x = train[cols]\n",
    "train.y = train[\"Label\"]\n",
    "# train.z = train[\"sub_categories\"]\n",
    "\n",
    "validate.x = validate[cols]\n",
    "validate.y = validate[\"Label\"]\n",
    "# validate.z = validate[\"sub_categories\"]\n",
    "\n",
    "test.x = test[cols]\n",
    "test.y = test[\"Label\"]\n",
    "# test.z = test[\"sub_categories\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "enjNN18smO33",
    "outputId": "b4a36f7d-ecc1-4052-c7f5-b4636f02dea2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18000, 1400) (18000,)\n",
      "(6000, 1400) (6000,)\n",
      "(6000, 1400) (6000,)\n",
      "       0     1     2     3     4     5     6     7     8     9     ...   1390  \\\n",
      "23765     0     0     0     0     0     0     0     0     0     0  ...      0   \n",
      "1035      0     0     1     0     0     0     0     0     0     0  ...      0   \n",
      "24968     0     0     1     0     0     0     0     0     0     0  ...      0   \n",
      "\n",
      "       1391  1392  1393  1394  1395  1396  1397  1398  1399  \n",
      "23765     0     0     0     0     0     2     0     0     0  \n",
      "1035      0     0     0     0     1     0     0     0     0  \n",
      "24968     0     0     0     0     1     0     0     0     0  \n",
      "\n",
      "[3 rows x 1400 columns]\n",
      "23765    info\n",
      "1035     info\n",
      "24968    info\n",
      "Name: Label, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Verify the shape of train.x, train.y\n",
    "print(train.x.shape, train.y.shape)\n",
    "print(validate.x.shape, validate.y.shape)\n",
    "print(test.x.shape, test.y.shape)\n",
    "# print(health_data.converse.map(len).max())\n",
    "\n",
    "print(train.x[:3])\n",
    "print(train.y[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MOag5RYVmO38",
    "outputId": "2bbbdc9b-0b50-4c84-f946-134cab9d2804"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "`sequences` must be a list of iterables. Found non-iterable: 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-9ef5e84024bb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mmax_review_length\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m500\u001b[0m \u001b[1;31m# This should be actually health_data.converse.map(len).max()\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msequence\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpad_sequences\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_review_length\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msequence\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpad_sequences\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_review_length\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras_preprocessing\\sequence.py\u001b[0m in \u001b[0;36mpad_sequences\u001b[1;34m(sequences, maxlen, dtype, padding, truncating, value)\u001b[0m\n\u001b[0;32m     59\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'__len__'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m             raise ValueError('`sequences` must be a list of iterables. '\n\u001b[1;32m---> 61\u001b[1;33m                              'Found non-iterable: ' + str(x))\n\u001b[0m\u001b[0;32m     62\u001b[0m         \u001b[0mlengths\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: `sequences` must be a list of iterables. Found non-iterable: 0"
     ]
    }
   ],
   "source": [
    "# Truncate and Pad input sequences\n",
    "\n",
    "max_review_length = 500 # This should be actually health_data.converse.map(len).max()\n",
    "train.x = sequence.pad_sequences(train.x, maxlen=max_review_length)\n",
    "test.x = sequence.pad_sequences(test.x, maxlen=max_review_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wXhGYUoXmO3_"
   },
   "source": [
    "### Let's inspect the first two sentences of train and their classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E5shUGlOmO4A",
    "outputId": "97de9e1a-2298-46c6-b023-0421bcee12d0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1905,   27,   24],\n",
       "       [   0, 2670,    0],\n",
       "       [  31,    9, 1334]], dtype=int64)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Simple Naive Bayes Model\n",
    "\n",
    "Mnb = MultinomialNB()\n",
    "Mnb.fit(train.x, train.y)\n",
    "preds_NB = Mnb.predict(test.x)\n",
    "confusion_matrix(test.y,preds_NB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZWncHeVgmO4D",
    "outputId": "d215fa57-2a27-472c-851b-f2db6cbf6c96"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9848333333333333 0.9816047662847566\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes Accuracy and recall\n",
    "\n",
    "accuracy = metrics.accuracy_score(test.y,preds_NB)\n",
    "recall = metrics.recall_score(test.y,preds_NB, average = 'macro')\n",
    "print(accuracy, recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SaMi93UlmO4G"
   },
   "source": [
    "### Inspect the same sentences after padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9SadWbo3mO4H",
    "outputId": "a90e6294-07cd-4013-f472-7c3d7f4d345e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ham' 'info' 'spam']\n"
     ]
    }
   ],
   "source": [
    "# Basic checks so that train.x, train.y,..,test.y are all in the same shape\n",
    "print(np.unique(train.y))\n",
    "# print(np.unique(train.z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mQcIDGdSmO4L",
    "outputId": "811738b7-985b-43bb-cb60-768c36b3d2b4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18000, 3)\n"
     ]
    }
   ],
   "source": [
    "# Convert the train.y into simple categorical variables 0,1,2\n",
    "train.y = pd.Categorical(train.y)\n",
    "train.y.codes\n",
    "train.y = to_categorical(np.asarray(train.y.codes))\n",
    "\n",
    "# Similarly convert the test.y into simple categorical variables 0,1,2\n",
    "test.y = pd.Categorical(test.y)\n",
    "test.y.codes\n",
    "test.y = to_categorical(np.asarray(test.y.codes))\n",
    "\n",
    "# Similarly convert the val.y into simple categorical variables 0,1,2\n",
    "validate.y = pd.Categorical(validate.y)\n",
    "validate.y.codes\n",
    "validate.y = to_categorical(np.asarray(validate.y.codes))\n",
    "\n",
    "print(train.y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bUcS-SCEmO4N",
    "outputId": "a551f474-fdb0-43ee-a85c-f05db149640f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18000, 3)\n"
     ]
    }
   ],
   "source": [
    "print(train.y.shape)\n",
    "# print(train.z[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RMO4ODj3mO4R"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SUNEEL\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:12: UserWarning: Pandas doesn't allow columns to be created via a new attribute name - see https://pandas.pydata.org/pandas-docs/stable/indexing.html#attribute-access\n",
      "  if sys.path[0] == '':\n"
     ]
    }
   ],
   "source": [
    "# Convert the train.y into simple categorical variables from 0,1,2,..,6\n",
    "train.z = pd.Categorical(train)\n",
    "train.z.codes\n",
    "train.z = to_categorical(np.asarray(train.z.codes))\n",
    "\n",
    "# Similarly convert the test.y into simple categorical variables from 0,1,2,..,6\n",
    "test.z = pd.Categorical(test)\n",
    "test.z.codes\n",
    "test.z = to_categorical(np.asarray(test.z.codes))\n",
    "\n",
    "# Similarly convert the val.y into simple categorical variables from 0,1,2,..,6\n",
    "validate.z = pd.Categorical(validate)\n",
    "validate.z.codes\n",
    "validate.z = to_categorical(np.asarray(validate.z.codes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JnSlHpn3mO4V",
    "outputId": "46c0f8cf-ef75-4b6d-855e-b2fe854972ad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18000, 1400) (18000, 3)\n",
      "<class 'pandas.core.frame.DataFrame'> <class 'numpy.ndarray'>\n",
      "(6000, 1400) (6000, 3)\n"
     ]
    }
   ],
   "source": [
    "# Verify the shape of train.x and train.y before feeding into the model\n",
    "print(train.x.shape, train.y.shape)\n",
    "print(type(train.x), type(train.y))\n",
    "\n",
    "print(validate.x.shape, validate.y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lKZZlt8emO4Y"
   },
   "outputs": [],
   "source": [
    "# Creating a custome \"Recall\" error function in Keras backend\n",
    "\n",
    "import keras.backend as K\n",
    "\n",
    "def recall(y_true, y_pred):\n",
    "    TP = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    PP = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = TP / (PP + K.epsilon())\n",
    "    return recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UmDd9ppKmO4b",
    "outputId": "67a2e58e-18c7-4293-a4e0-a84298ac8f12"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 128)               179328    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 387       \n",
      "=================================================================\n",
      "Total params: 179,715\n",
      "Trainable params: 179,715\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Simple MLP, SGD and Dropout [6 categories]\n",
    "\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(128, input_dim=1400, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy', recall])\n",
    "\n",
    "# sgd = SGD(lr=0.04, decay=1e-6, momentum=0.6, nesterov=True)\n",
    "# model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy']) 0.82% test accuracy\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YqS_6fOemO4f",
    "outputId": "0e0c8624-82f3-4037-c06e-a0ff74f49ff7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SUNEEL\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 18000 samples, validate on 6000 samples\n",
      "Epoch 1/15\n",
      " - 6s - loss: 0.0933 - acc: 0.9738 - recall: 0.9500 - val_loss: 0.0146 - val_acc: 0.9975 - val_recall: 0.9973\n",
      "Epoch 2/15\n",
      " - 3s - loss: 0.0093 - acc: 0.9979 - recall: 0.9979 - val_loss: 0.0113 - val_acc: 0.9977 - val_recall: 0.9973\n",
      "Epoch 3/15\n",
      " - 3s - loss: 0.0049 - acc: 0.9986 - recall: 0.9986 - val_loss: 0.0113 - val_acc: 0.9970 - val_recall: 0.9970\n",
      "Epoch 4/15\n",
      " - 3s - loss: 0.0027 - acc: 0.9993 - recall: 0.9993 - val_loss: 0.0110 - val_acc: 0.9972 - val_recall: 0.9972\n",
      "Epoch 5/15\n",
      " - 3s - loss: 0.0019 - acc: 0.9996 - recall: 0.9996 - val_loss: 0.0114 - val_acc: 0.9972 - val_recall: 0.9972\n",
      "Epoch 6/15\n",
      " - 3s - loss: 0.0020 - acc: 0.9996 - recall: 0.9996 - val_loss: 0.0109 - val_acc: 0.9975 - val_recall: 0.9975\n",
      "Epoch 7/15\n",
      " - 3s - loss: 0.0016 - acc: 0.9997 - recall: 0.9997 - val_loss: 0.0113 - val_acc: 0.9977 - val_recall: 0.9977\n",
      "Epoch 8/15\n",
      " - 3s - loss: 0.0014 - acc: 0.9998 - recall: 0.9998 - val_loss: 0.0128 - val_acc: 0.9973 - val_recall: 0.9972\n",
      "Epoch 9/15\n",
      " - 3s - loss: 0.0011 - acc: 0.9998 - recall: 0.9998 - val_loss: 0.0147 - val_acc: 0.9972 - val_recall: 0.9970\n",
      "Epoch 10/15\n",
      " - 3s - loss: 0.0010 - acc: 0.9999 - recall: 0.9999 - val_loss: 0.0155 - val_acc: 0.9975 - val_recall: 0.9973\n",
      "Epoch 11/15\n",
      " - 3s - loss: 8.9528e-04 - acc: 0.9999 - recall: 0.9999 - val_loss: 0.0151 - val_acc: 0.9970 - val_recall: 0.9970\n",
      "Epoch 12/15\n",
      " - 3s - loss: 0.0011 - acc: 0.9998 - recall: 0.9998 - val_loss: 0.0156 - val_acc: 0.9973 - val_recall: 0.9972\n",
      "Epoch 13/15\n",
      " - 3s - loss: 8.1365e-04 - acc: 0.9998 - recall: 0.9998 - val_loss: 0.0182 - val_acc: 0.9975 - val_recall: 0.9975\n",
      "Epoch 14/15\n",
      " - 3s - loss: 9.9928e-04 - acc: 0.9998 - recall: 0.9998 - val_loss: 0.0151 - val_acc: 0.9973 - val_recall: 0.9972\n",
      "Epoch 15/15\n",
      " - 3s - loss: 7.6495e-04 - acc: 0.9999 - recall: 0.9999 - val_loss: 0.0172 - val_acc: 0.9970 - val_recall: 0.9970\n",
      "Accurcay_mlp_cat: 99.62%\n",
      "Recall_mlp_cat: 99.62%\n"
     ]
    }
   ],
   "source": [
    "# Fit the model for y = categories\n",
    "model.fit(np.array(train.x), train.y, nb_epoch=15, batch_size=32, verbose=2, validation_data=(np.array(validate.x), validate.y))\n",
    "\n",
    "scores_mlp = model.evaluate(np.array(test.x), test.y, verbose=0)\n",
    "print(\"Accurcay_mlp_cat: %.2f%%\" % (scores_mlp[1]*100))\n",
    "print(\"Recall_mlp_cat: %.2f%%\" % (scores_mlp[2]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MUGrnC5fmO4j",
    "outputId": "6d9ea7b7-71d2-4218-ea45-a1819051c16b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_9 (Dense)              (None, 128)               640128    \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 21)                2709      \n",
      "=================================================================\n",
      "Total params: 642,837\n",
      "Trainable params: 642,837\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SUNEEL\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:14: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  \n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected dense_9_input to have shape (5000,) but got array with shape (1400,)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-47-bba25358518f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;31m# Fit the model for y = categories\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnb_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[0mscores_mlp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m    948\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    949\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 950\u001b[1;33m             batch_size=batch_size)\n\u001b[0m\u001b[0;32m    951\u001b[0m         \u001b[1;31m# Prepare validation data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    952\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[0;32m    747\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    748\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# Don't enforce the batch size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 749\u001b[1;33m             exception_prefix='input')\n\u001b[0m\u001b[0;32m    750\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    751\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    135\u001b[0m                             \u001b[1;34m': expected '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' to have shape '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' but got array with shape '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 137\u001b[1;33m                             str(data_shape))\n\u001b[0m\u001b[0;32m    138\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking input: expected dense_9_input to have shape (5000,) but got array with shape (1400,)"
     ]
    }
   ],
   "source": [
    "# Simple MLP, SGD and Dropout [21 sub_categories]\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(128, input_dim=5000, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(21, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy', recall])\n",
    "\n",
    "# sgd = SGD(lr=0.04, decay=1e-6, momentum=0.6, nesterov=True)\n",
    "# model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy', recall])\n",
    "print(model.summary())\n",
    "\n",
    "# Fit the model for y = categories\n",
    "model.fit(np.array(train.x), train.y, nb_epoch=20, batch_size=32, verbose=2, validation_data=(np.array(validate.x), validate.y))\n",
    "\n",
    "scores_mlp = model.evaluate(np.array(test.x), test.z, verbose=0)\n",
    "print(\"Accurcay_mlp_sub_cat: %.2f%%\" % (scores_mlp[1]*100))\n",
    "print(\"Recall_mlp_sub_cat: %.2f%%\" % (scores_mlp[2]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RiiIdH26mO4n",
    "outputId": "6b8c1822-6812-4106-d6b5-92008bd9b45d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 1400, 32)          16000     \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 100)               53200     \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 3)                 303       \n",
      "=================================================================\n",
      "Total params: 69,503\n",
      "Trainable params: 69,503\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SUNEEL\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:14: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 18000 samples, validate on 6000 samples\n",
      "Epoch 1/10\n",
      "18000/18000 [==============================] - 1043s 58ms/step - loss: 0.9424 - acc: 0.5866 - recall: 0.5036 - val_loss: 0.7611 - val_acc: 0.7038 - val_recall: 0.2300\n",
      "Epoch 2/10\n",
      "18000/18000 [==============================] - 957s 53ms/step - loss: 0.6728 - acc: 0.7342 - recall: 0.3946 - val_loss: 0.6571 - val_acc: 0.7425 - val_recall: 0.6425\n",
      "Epoch 3/10\n",
      "18000/18000 [==============================] - 1026s 57ms/step - loss: 0.5919 - acc: 0.7721 - recall: 0.4084 - val_loss: 0.5501 - val_acc: 0.7928 - val_recall: 0.4838\n",
      "Epoch 4/10\n",
      "18000/18000 [==============================] - 1024s 57ms/step - loss: 0.5029 - acc: 0.8143 - recall: 0.6405 - val_loss: 0.4344 - val_acc: 0.8213 - val_recall: 0.5463\n",
      "Epoch 5/10\n",
      "18000/18000 [==============================] - 1119s 62ms/step - loss: 0.4325 - acc: 0.8287 - recall: 0.6659 - val_loss: 0.4178 - val_acc: 0.8278 - val_recall: 0.6208\n",
      "Epoch 6/10\n",
      "18000/18000 [==============================] - 1226s 68ms/step - loss: 0.4574 - acc: 0.8244 - recall: 0.6633 - val_loss: 0.4391 - val_acc: 0.8287 - val_recall: 0.6527\n",
      "Epoch 7/10\n",
      "18000/18000 [==============================] - 1063s 59ms/step - loss: 0.4571 - acc: 0.8302 - recall: 0.6449 - val_loss: 0.3828 - val_acc: 0.8505 - val_recall: 0.5747\n",
      "Epoch 8/10\n",
      "18000/18000 [==============================] - 974s 54ms/step - loss: 0.3577 - acc: 0.8602 - recall: 0.6084 - val_loss: 0.3639 - val_acc: 0.8673 - val_recall: 0.4438\n",
      "Epoch 9/10\n",
      "18000/18000 [==============================] - 1317s 73ms/step - loss: 0.3374 - acc: 0.8686 - recall: 0.6622 - val_loss: 0.4306 - val_acc: 0.8375 - val_recall: 0.6625\n",
      "Epoch 10/10\n",
      "18000/18000 [==============================] - 1133s 63ms/step - loss: 0.3145 - acc: 0.8762 - recall: 0.7007 - val_loss: 0.3587 - val_acc: 0.8367 - val_recall: 0.5703\n",
      "Accuracy_lstm_cat: 83.88%\n",
      "Recall_lstm_cat: 57.20%\n"
     ]
    }
   ],
   "source": [
    "# LSTM Model [3 categories]\n",
    "\n",
    "top_words = 500\n",
    "embedding_vecor_length = 32\n",
    "max_converse_length = 500\n",
    "# max_converse_length = health_data.converse.map(len).max() # This gives the max length of the converse, used 500 here\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, embedding_vecor_length, input_length=1400))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(3, activation='sigmoid'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy', recall])\n",
    "print(model.summary())\n",
    "\n",
    "model.fit(np.array(train.x), train.y, nb_epoch=10, batch_size=64, validation_data=(np.array(validate.x), validate.y))\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(np.array(test.x), test.y, verbose=0)\n",
    "print(\"Accuracy_lstm_cat: %.2f%%\" % (scores[1]*100))\n",
    "print(\"Recall_lstm_cat: %.2f%%\" % (scores[2]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "A1pjMeG-mO4u"
   },
   "outputs": [],
   "source": [
    "# # LSTM Model [21 categories]\n",
    "\n",
    "# top_words = 500\n",
    "# embedding_vecor_length = 32\n",
    "# max_converse_length = 500\n",
    "# # max_converse_length = health_data.converse.map(len).max() # This gives the max length of the converse, used 500 here\n",
    "# model = Sequential()\n",
    "# model.add(Embedding(top_words, embedding_vecor_length, input_length=500))\n",
    "# model.add(LSTM(100))\n",
    "# model.add(Dense(21, activation='sigmoid'))\n",
    "# model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy', recall])\n",
    "# print(model.summary())\n",
    "\n",
    "# model.fit(np.array(train.x), train.z, nb_epoch=1, batch_size=64, validation_data=(np.array(val.x), val.z))\n",
    "# # Final evaluation of the model\n",
    "# scores = model.evaluate(np.array(test.x), test.z, verbose=0)\n",
    "# print(\"Accuracy_lstm_sub_cat: %.2f%%\" % (scores[1]*100))\n",
    "# print(\"Recall_lstm_sub_cat: %.2f%%\" % (scores[2]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "3HIGd4x-mO4x"
   },
   "outputs": [],
   "source": [
    "# CNN 1D \n",
    "\n",
    "# set parameters:\n",
    "max_features = 500\n",
    "maxlen = 1400\n",
    "batch_size = 32\n",
    "embedding_dims = 50\n",
    "filters = 250\n",
    "kernel_size = 3\n",
    "hidden_dims = 250\n",
    "epochs = 10 #Models computational time is a lot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3uLPndnxmO40",
    "outputId": "5fc2d4b6-ed9f-4dd2-9c05-a20589645b1e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build Convolution 1D model...\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, 1400, 50)          25000     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 1400, 50)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 1398, 250)         37750     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 250)               62750     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 3)                 753       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 3)                 0         \n",
      "=================================================================\n",
      "Total params: 126,253\n",
      "Trainable params: 126,253\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nfsroot/data/home/s_jaysetty/.local/lib/python2.7/site-packages/ipykernel_launcher.py:21: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(250, 3, padding=\"valid\", strides=1, activation=\"relu\")`\n"
     ]
    }
   ],
   "source": [
    "# CNN 1D [3 categories]\n",
    "\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D\n",
    "\n",
    "print('Build Convolution 1D model...')\n",
    "model = Sequential()\n",
    "\n",
    "# we start off with an efficient embedding layer which maps\n",
    "# our vocab indices into embedding_dims dimensions\n",
    "model.add(Embedding(max_features,\n",
    "                    embedding_dims,\n",
    "                    input_length=maxlen))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "# we add a Convolution1D, which will learn filters\n",
    "# word group filters of size filter_length:\n",
    "model.add(Conv1D(filters,\n",
    "                 kernel_size,\n",
    "                 border_mode='valid',\n",
    "                 activation='relu',\n",
    "                 subsample_length=1))\n",
    "# we use max pooling:\n",
    "model.add(GlobalMaxPooling1D())\n",
    "\n",
    "# We add a vanilla hidden layer:\n",
    "model.add(Dense(hidden_dims))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# We project onto a single unit output layer, and squash it with a sigmoid:\n",
    "model.add(Dense(3))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy', recall])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "67xCIbYcmO44",
    "outputId": "f29e0676-618c-4771-daae-e652576d03bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 18000 samples, validate on 6000 samples\n",
      "Epoch 1/10\n",
      "18000/18000 [==============================] - 8s - loss: 0.5914 - acc: 0.7303 - recall: 0.1734 - val_loss: 1.3650 - val_acc: 0.4250 - val_recall: 0.3150\n",
      "Epoch 2/10\n",
      "18000/18000 [==============================] - 7s - loss: 0.4518 - acc: 0.8038 - recall: 0.1626 - val_loss: 2.2774 - val_acc: 0.3662 - val_recall: 0.3178\n",
      "Epoch 3/10\n",
      "18000/18000 [==============================] - 7s - loss: 0.4269 - acc: 0.8149 - recall: 0.1879 - val_loss: 3.1005 - val_acc: 0.3387 - val_recall: 0.3262\n",
      "Epoch 4/10\n",
      "18000/18000 [==============================] - 6s - loss: 0.4132 - acc: 0.8196 - recall: 0.2393 - val_loss: 4.3311 - val_acc: 0.3315 - val_recall: 0.3262\n",
      "Epoch 5/10\n",
      "18000/18000 [==============================] - 6s - loss: 0.4065 - acc: 0.8206 - recall: 0.2596 - val_loss: 5.0334 - val_acc: 0.3347 - val_recall: 0.3262\n",
      "Epoch 6/10\n",
      "18000/18000 [==============================] - 6s - loss: 0.3984 - acc: 0.8266 - recall: 0.2358 - val_loss: 4.9216 - val_acc: 0.3363 - val_recall: 0.3260\n",
      "Epoch 7/10\n",
      "18000/18000 [==============================] - 6s - loss: 0.4006 - acc: 0.8259 - recall: 0.2864 - val_loss: 5.4329 - val_acc: 0.3343 - val_recall: 0.3260\n",
      "Epoch 8/10\n",
      "18000/18000 [==============================] - 6s - loss: 0.3925 - acc: 0.8282 - recall: 0.2225 - val_loss: 6.3290 - val_acc: 0.3418 - val_recall: 0.3260\n",
      "Epoch 9/10\n",
      "18000/18000 [==============================] - 6s - loss: 0.3910 - acc: 0.8282 - recall: 0.2204 - val_loss: 6.1687 - val_acc: 0.3537 - val_recall: 0.3260\n",
      "Epoch 10/10\n",
      "18000/18000 [==============================] - 6s - loss: 0.3785 - acc: 0.8372 - recall: 0.2087 - val_loss: 6.7315 - val_acc: 0.3318 - val_recall: 0.3260\n",
      "Accuracy_cnn1d_cat: 34.48%\n",
      "Recall_cnn1d_cat: 33.75%\n"
     ]
    }
   ],
   "source": [
    "model.fit(np.array(train.x), train.y, batch_size=batch_size, nb_epoch=epochs, validation_data=(np.array(validate.x), validate.y) )\n",
    "\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(np.array(test.x), test.y, verbose=0)\n",
    "print(\"Accuracy_cnn1d_cat: %.2f%%\" % (scores[1]*100))\n",
    "print(\"Recall_cnn1d_cat: %.2f%%\" % (scores[2]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "mE42C3vymO4_"
   },
   "outputs": [],
   "source": [
    "# # CNN 1D [21 categories]\n",
    "\n",
    "# from keras.layers import Conv1D, GlobalMaxPooling1D\n",
    "\n",
    "# print('Build Convolution 1D model...')\n",
    "# model = Sequential()\n",
    "\n",
    "# # we start off with an efficient embedding layer which maps\n",
    "# # our vocab indices into embedding_dims dimensions\n",
    "# model.add(Embedding(max_features,\n",
    "#                     embedding_dims,\n",
    "#                     input_length=maxlen))\n",
    "# model.add(Dropout(0.2))\n",
    "\n",
    "# # we add a Convolution1D, which will learn filters\n",
    "# # word group filters of size filter_length:\n",
    "# model.add(Conv1D(filters,\n",
    "#                  kernel_size,\n",
    "#                  border_mode='valid',\n",
    "#                  activation='relu',\n",
    "#                  subsample_length=1))\n",
    "# # we use max pooling:\n",
    "# model.add(GlobalMaxPooling1D())\n",
    "\n",
    "# # We add a vanilla hidden layer:\n",
    "# model.add(Dense(hidden_dims))\n",
    "# model.add(Dropout(0.2))\n",
    "# model.add(Activation('relu'))\n",
    "\n",
    "# # We project onto a single unit output layer, and squash it with a sigmoid:\n",
    "# model.add(Dense(21))\n",
    "# model.add(Activation('sigmoid'))\n",
    "\n",
    "# model.summary()\n",
    "\n",
    "# model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy', recall])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "-N4wC0snmO5J"
   },
   "outputs": [],
   "source": [
    "# model.fit(np.array(train.x), train.z, batch_size=batch_size, nb_epoch=epochs, validation_data=(np.array(val.x), val.z) )\n",
    "\n",
    "# # Final evaluation of the model\n",
    "# scores = model.evaluate(np.array(test.x), test.z, verbose=0)\n",
    "# print(\"Accuracy_cnn1d_sub_cat: %.2f%%\" % (scores[1]*100))\n",
    "# print(\"Recall_cnn1d_sub_cat: %.2f%%\" % (scores[2]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "ovoApoCumO5O"
   },
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# model_mlp = model.fit(np.array(train.x), train.z, nb_epoch=20, batch_size=32, verbose=2, validation_data=(np.array(val.x), val.z))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "95B6-1ssmO5X"
   },
   "outputs": [],
   "source": [
    "# history = model_mlp\n",
    "# # list all data in history\n",
    "# print(history.history.keys())\n",
    "# # summarize history for accuracy\n",
    "# plt.plot(history.history['acc'])\n",
    "# plt.plot(history.history['val_acc'])\n",
    "# plt.title('model accuracy')\n",
    "# plt.ylabel('accuracy')\n",
    "# plt.xlabel('epoch')\n",
    "# plt.legend(['train', 'test'], loc='upper left')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "X2Q6hpu2mO5e"
   },
   "outputs": [],
   "source": [
    "# # summarize history for loss\n",
    "# plt.plot(history.history['loss'])\n",
    "# plt.plot(history.history['val_loss'])\n",
    "# plt.title('model loss')\n",
    "# plt.ylabel('loss')\n",
    "# plt.xlabel('epoch')\n",
    "# plt.legend(['train', 'test'], loc='upper left')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "ckbKDMbmmO5k"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "sms.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
